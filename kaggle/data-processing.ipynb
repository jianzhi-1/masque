{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Set up","metadata":{}},{"cell_type":"code","source":"!pip install librosa\n!pip install soundfile\n!pip install speechbrain","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport librosa\nimport matplotlib.pyplot as plt\nimport soundfile as sf\n\nimport logging\n\nfrom speechbrain.inference.vocoders import HIFIGAN\nfrom speechbrain.inference.TTS import Tacotron2\nfrom speechbrain.lobes.models.FastSpeech2 import mel_spectogram\n\nimport IPython.display as ipd\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn.utils.rnn import pad_sequence\nimport torchaudio\n\nfrom datasets import load_dataset # Expresso dataset\nimport tqdm.notebook","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Utils","metadata":{}},{"cell_type":"code","source":"EPS = 1e-6\n\ndef equals(a, b):\n    return abs(a - b) < EPS\n\ndef dtw(a, b):\n    n, m = a.shape[0], b.shape[0]\n    dtw_matrix = np.full((n + 1, m + 1), np.inf)\n    dtw_matrix[0, 0] = 0\n\n    for i in range(1, n + 1):\n        for j in range(1, m + 1):\n            cost = np.linalg.norm(a[i - 1] - b[j - 1])  # Euclidean distance\n            dtw_matrix[i, j] = cost + min(dtw_matrix[i - 1, j],    # Insertion\n                                           dtw_matrix[i, j - 1],    # Deletion\n                                           dtw_matrix[i - 1, j - 1]) # Match\n\n    # Backtrack to find the optimal path\n    i, j = n, m\n    path = []\n\n    while i > 0 or j > 0:\n        path.append((i - 1, j - 1))\n        if i > 0 and j > 0:\n            if equals(dtw_matrix[i, j], dtw_matrix[i - 1, j - 1] + np.linalg.norm(a[i - 1] - b[j - 1])):\n                i -= 1\n                j -= 1\n            elif equals(dtw_matrix[i, j], dtw_matrix[i - 1, j] + np.linalg.norm(a[i - 1] - b[j - 1])):\n                i -= 1\n            else:\n                j -= 1\n        elif i > 0:\n            i -= 1\n        else:\n            j -= 1\n\n    path.reverse()\n    return dtw_matrix[n, m], path\n\ndef load_audio(file_path):\n    y, sr = librosa.load(file_path, sr=None)\n    return y, sr\n\ndef align(signal_a, signal_b, path):\n    aligned_b = np.zeros_like(signal_a)\n\n    for idx_a, idx_b in path:\n        aligned_b[idx_a] = signal_b[idx_b]\n\n    return aligned_b\n\ndef main(audio_file_1, audio_file_2):\n    \n    # 0. Load audio files\n    audio_a, sr_a = load_audio(audio_file_1)\n    audio_b, sr_b = load_audio(audio_file_2)\n\n    # 1. Extract MFCC features\n    mfcc_a = librosa.feature.mfcc(y=audio_a, sr=sr_a, n_mfcc=13).T\n    mfcc_b = librosa.feature.mfcc(y=audio_b, sr=sr_b, n_mfcc=13).T\n\n    # 2. Normalise MFCC features\n    mfcc_a_normalised = (mfcc_a - np.mean(mfcc_a, axis=0))/(np.std(mfcc_a, axis=0))\n    mfcc_b_normalised = (mfcc_b - np.mean(mfcc_b, axis=0))/(np.std(mfcc_b, axis=0))\n\n    # 3. Perform DTW\n    _, path = dtw(mfcc_a_normalised, mfcc_b_normalised)\n\n    # 4. Align audio_b using DTW path\n    mfcc_b_aligned = align_mfcc(mfcc_a_normalised, mfcc_b, path)\n    audio_b_aligned = librosa.feature.inverse.mfcc_to_audio(np.einsum(\"ij->ji\", mfcc_b_aligned))\n\n    # 5. Export\n    sf.write(f'./{audio_file_2}_aligned.wav', audio_b_aligned, sr_b)\n    print(f\"Aligned audio saved as '{audio_file_2}_aligned.wav'.\")\n\n    return\n\ndef naive_cut(audio_file_1, audio_file_2):\n    audio_a, _ = load_audio(audio_file_1)\n    audio_b, sr_b = load_audio(audio_file_2)\n    sf.write('./audio_b_cut.wav', audio_b[:len(audio_a)], sr_b)\n    print(\"Aligned audio saved as 'audio_b_cut.wav'.\")\n\ndef naive_speed(audio_file_1, audio_file_2):\n    audio_a, sr_a = load_audio(audio_file_1)\n    audio_b, _ = load_audio(audio_file_2)\n    sf.write('./audio_b_speed.wav', audio_b, int(sr_a*len(audio_b)/len(audio_a)))\n    print(\"Aligned audio saved as 'audio_b_speed.wav'.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def view_spectrogram(spectrogram, title=\"Mel Spectrogram\", n_mels=80):\n    if isinstance(spectrogram, np.ndarray):\n        spectrogram = torch.tensor(spectrogram)\n    if spectrogram.shape[0] != 80:\n        spectrogram = torch.einsum(\"ij->ji\", spectrogram)\n    assert spectrogram.shape[0] == n_mels, f\"spectrogram shape {spectrogram.shape} != ({n_mels}, seq_length)\"\n    print(spectrogram.shape)\n    plt.figure(figsize=(10, 4))\n    librosa.display.specshow(spectrogram, sr=22050, x_axis='time', y_axis='mel', fmax=8000)\n    plt.colorbar(format='%+2.0f dB')\n    plt.title(title)\n    plt.tight_layout()\n    plt.show()\n\ndef get_spectrogram(file_name):\n\n    signal, rate = torchaudio.load(file_name)\n    signal = torchaudio.functional.resample(signal, orig_freq=rate, new_freq=22050)\n\n    spectrogram, _ = mel_spectogram(\n        audio=signal.squeeze(),\n        sample_rate=22050,\n        hop_length=256,\n        win_length=None,\n        n_mels=80,\n        n_fft=1024,\n        f_min=0.0,\n        f_max=8000.0,\n        power=1,\n        normalized=False,\n        min_max_energy_norm=True,\n        norm=\"slaney\",\n        mel_scale=\"slaney\",\n        compression=True\n    )\n\n    return spectrogram\n\ndef get_spectrogram_from_waveform(signal, rate):\n    \n    if isinstance(signal, np.ndarray):\n        signal = torch.tensor(signal, dtype=torch.float32)\n    \n    signal = torchaudio.functional.resample(signal, orig_freq=rate, new_freq=22050)\n\n    spectrogram, _ = mel_spectogram(\n        audio=signal.squeeze(),\n        sample_rate=22050,\n        hop_length=256,\n        win_length=None,\n        n_mels=80,\n        n_fft=1024,\n        f_min=0.0,\n        f_max=8000.0,\n        power=1,\n        normalized=False,\n        min_max_energy_norm=True,\n        norm=\"slaney\",\n        mel_scale=\"slaney\",\n        compression=True\n    )\n\n    return spectrogram\n\ndef spectrogram_to_waveform(spectrogram, save_file_name):\n    waveforms = hifi_gan.decode_batch(spectrogram) # spectrogram to waveform\n    torchaudio.save(save_file_name, waveforms.squeeze(1), 22050)\n\ndef get_reconstructed_sample(file_name, save_file_name):\n\n    signal, rate = torchaudio.load(file_name)\n    signal = torchaudio.functional.resample(signal, orig_freq=rate, new_freq=22050)\n\n    spectrogram, _ = mel_spectogram(\n        audio=signal.squeeze(),\n        sample_rate=22050,\n        hop_length=256,\n        win_length=None,\n        n_mels=80,\n        n_fft=1024,\n        f_min=0.0,\n        f_max=8000.0,\n        power=1,\n        normalized=False,\n        min_max_energy_norm=True,\n        norm=\"slaney\",\n        mel_scale=\"slaney\",\n        compression=True\n    )\n\n    waveforms = hifi_gan.decode_batch(spectrogram) # spectrogram to waveform\n\n    torchaudio.save(save_file_name, waveforms.squeeze(1), 22050)\n\ndef transcript_to_audio(sentence, save_file_name):\n    \n    mel_output, mel_length, alignment = tacotron2.encode_text(sentence)\n    # 1. Mel spectrogram with properties in the Tacotron paper (or see get_reconstructed_sample)\n    #    Shape = (batch_size, n_mels=80, Mel_length + 1); Mel_length proportional to length of sequence\n    # 2. Mel_length = mel_output.shape[2] - 1\n    # 3. Alignment\n    #    Shape = (batch_size, Mel_length, Token_length) where Token_length is from tacotron2.text_to_seq(txt)\n\n    waveforms = hifi_gan.decode_batch(mel_output) # spectrogram to waveform\n\n    torchaudio.save(save_file_name, waveforms.squeeze(1), 22050)\n\ndef transcript_to_mel(sentence):\n    mel_output, mel_length, alignment = tacotron2.encode_text(sentence)\n    return mel_output.squeeze() # remove the batch dimension\n\ndef mel_to_audio(mel_output, save_file_name=None, display=False):\n    if isinstance(mel_output, np.ndarray):\n        mel_output = torch.tensor(mel_output)\n    if mel_output.shape[0] != 80:\n        mel_output = torch.einsum(\"ij->ji\", mel_output)\n    waveforms = hifi_gan.decode_batch(mel_output) # spectrogram to waveform\n    if save_file_name is not None: torchaudio.save(save_file_name, waveforms.squeeze(1), 22050)\n    if display: return ipd.Audio(waveforms, rate=22050)\n    return waveforms\n\ndef sample_audio(dataset, idx:int):\n    print(dataset[idx])\n    mel_to_audio(torch.einsum(\"ij->ji\", dataset[idx][\"data_mel\"]), f\"sample_{idx}.wav\")\n    mel_to_audio(torch.einsum(\"ij->ji\", torch.tensor(dataset[idx][\"original_data_mel\"])), f\"sample_{idx}_original.wav\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data and Pre-trained Models","metadata":{}},{"cell_type":"code","source":"tacotron2 = Tacotron2.from_hparams(source=\"speechbrain/tts-tacotron2-ljspeech\", savedir=\"tmpdir_tts\")\nhifi_gan = HIFIGAN.from_hparams(source=\"speechbrain/tts-hifigan-ljspeech\", savedir=\"tmpdir_vocoder\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset = load_dataset(\"ylacombe/expresso\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[\"train\"][0] # visualise data","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(dataset[\"train\"]) # 11615","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MelSpectrogramDataset(torch.utils.data.Dataset):\n    def __init__(self, split, source, params):\n        assert split in (\"train\", \"valid\", \"test\"), \"invalid split\"\n        self.source = source.shuffle(seed=42) # for reproducibility\n        self.base_pointer, self.limit_pointer = params[split]\n        self.cache = dict()\n        self.label_encoder = {\n            'confused': 0,\n            'default': 1,\n            'emphasis': 2,\n            'enunciated': 3,\n            'essentials': 4,\n            'happy': 5,\n            'laughing': 6,\n            'longform': 7,\n            'sad': 8,\n            'singing': 9,\n            'whisper': 10\n        }\n        self.speaker_encoder = {\n            'ex01': 0, \n            'ex02': 1,\n            'ex03': 2,\n            'ex04': 3\n        }\n\n    def __len__(self):\n        return self.limit_pointer - self.base_pointer\n\n    def __getitem__(self, idx):\n        \n        # 0. Preprocessing\n        \n        idx += self.base_pointer\n        assert idx < self.limit_pointer, \"index out of bounds\"\n        if idx in self.cache: return self.cache[idx] # memoisation\n        \n        item = self.source[idx]\n        \n        label = item[\"style\"]\n        speaker = item[\"speaker_id\"]\n        \n        # 1. Obtain Mel Spectrograms\n        \n        data_mel_spectrogram = get_spectrogram_from_waveform(item[\"audio\"][\"array\"], item[\"audio\"][\"sampling_rate\"])\n        ai_mel_spectrogram = transcript_to_mel(item[\"text\"])\n        \n        data_mel_spectrogram = np.einsum(\"ij->ji\", data_mel_spectrogram)\n        ai_mel_spectrogram = np.einsum(\"ij->ji\", ai_mel_spectrogram)\n        \n        assert ai_mel_spectrogram.shape[1] == 80\n        assert data_mel_spectrogram.shape[1] == 80\n        \n        # 2. DTW\n        dtw_cost, path = dtw(ai_mel_spectrogram, data_mel_spectrogram)\n        aligned_to_ai_spectrogram = align(ai_mel_spectrogram, data_mel_spectrogram, path)\n        \n        assert aligned_to_ai_spectrogram.shape == ai_mel_spectrogram.shape, \"DTW was not successful\"\n        assert aligned_to_ai_spectrogram.shape[1] == 80\n        \n        # 3. Duration modelling\n        \n        duration_arr = np.zeros(len(ai_mel_spectrogram))\n        for i, (x, y) in enumerate(path):\n            if i == 0:\n                duration_arr[x] += 1\n            else:\n                xp, yp = path[i - 1]\n                if yp == y:\n                    duration_arr[xp] -= 1\n                    duration_arr[x] += 1\n                else:\n                    duration_arr[x] += 1\n        assert sum(duration_arr) == len(data_mel_spectrogram), \"duration modelling not successful\"\n        \n        # 4. Return AI Mel, aligned Data Mel, Emotion Label, Speaker Label, original Data Mel\n        self.cache[idx] = {\n            \"ai_mel\": torch.tensor(ai_mel_spectrogram), \n            \"data_mel\": torch.tensor(aligned_to_ai_spectrogram), \n            \"label\": torch.tensor([self.label_encoder[label]]), \n            \"speaker\": torch.tensor([self.speaker_encoder[speaker]]), \n            \"original_data_mel\": torch.tensor(data_mel_spectrogram),\n            \"sequence_length\": torch.tensor([ai_mel_spectrogram.shape[0]]),\n            \"duration\": torch.tensor(duration_arr),\n            \"text\": item[\"text\"],\n            \"data_audio\": item[\"audio\"][\"array\"],\n            \"data_sample_rate\": item[\"audio\"][\"sampling_rate\"],\n            \"ai_audio\": mel_to_audio(ai_mel_spectrogram),\n            \"ai_sample_rate\": 22050\n        }\n        return self.cache[idx]\n    \n    @staticmethod\n    def collate(batch):\n        \n        assert torch.cuda.is_available()\n        device = torch.device(\"cuda\")\n        \n        ai_mel = pad_sequence(\n            [item[\"ai_mel\"] for item in batch],\n            batch_first=True, padding_value=np.nan\n        )\n        data_mel = pad_sequence(\n            [item[\"data_mel\"] for item in batch],\n            batch_first=True, padding_value=np.nan\n        )\n        duration = pad_sequence(\n            [item[\"duration\"] for item in batch],\n            batch_first=True, padding_value=np.nan\n        )\n        labels = torch.cat(tuple([item[\"label\"] for item in batch]))\n        sequence_lengths = torch.cat(tuple([item[\"sequence_length\"] for item in batch]))\n        mask = torch.all(torch.where(torch.isnan(ai_mel), torch.full(ai_mel.shape, True), torch.full(ai_mel.shape, False)), 2)\n        mask_check = torch.all(torch.where(torch.isnan(data_mel), torch.full(data_mel.shape, True), torch.full(data_mel.shape, False)), 2)\n        mask_double_check = torch.where(torch.isnan(duration), torch.full(duration.shape, True), torch.full(duration.shape, False))\n        assert torch.equal(mask, mask_check), \"mask is dubious\"\n        assert torch.equal(mask, mask_double_check), f\"mask is dubious {mask.shape}, {mask_double_check.shape}\"\n        \n        batch_size = len(batch)\n        _, ai_mel_max_length, _ = ai_mel.shape\n        assert ai_mel.shape == (batch_size, ai_mel_max_length, 80)\n        assert data_mel.shape == ai_mel.shape\n        assert duration.shape == ai_mel.shape[:2]\n        assert sequence_lengths.shape == torch.Size([batch_size])\n        assert torch.all(sequence_lengths > 0), \"not all sequence lengths are positive\"\n        assert mask.shape == ai_mel.shape[:2]\n        \n        return {\n            \"ai_mel\": ai_mel.to(device),\n            \"data_mel\": data_mel.to(device), \n            \"labels\": labels.to(device),\n            \"sequence_length\": sequence_lengths.to(device),\n            \"mask\": mask.to(device),\n            \"duration\": duration.to(device)\n        }","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Data Processing","metadata":{}},{"cell_type":"code","source":"preprocessed = MelSpectrogramDataset(\"train\", dataset[\"train\"], {\n    \"train\": (0, 7000),\n    \"valid\": (7000, 10000),\n    \"test\": (10000, 11615)\n})","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def cache_dataset(source, params, save_file_name=\"alldata.pth\"):\n    train_dataset = MelSpectrogramDataset(\"train\", source, params)\n    train_ls = []\n    for i in tqdm(range(len(train_dataset))):\n        train_ls.append(train_dataset[i])\n    torch.save({\"training_data\": train_ls}, f\"train_{save_file_name}\")\n    print(\"finished train\")\n    \n    validation_dataset = MelSpectrogramDataset(\"valid\", source, params)\n    valid_ls = []\n    for i in tqdm(range(len(validation_dataset))):\n        valid_ls.append(validation_dataset[i])\n    torch.save({\"validation_data\": valid_ls}, f\"valid_{save_file_name}\")\n    print(\"finished valid\")\n    \n    test_dataset = MelSpectrogramDataset(\"valid\", source, params)\n    test_ls = []\n    for i in tqdm(range(len(test_dataset))):\n        test_ls.append(test_dataset[i])\n    torch.save({\"testing_data\": test_ls}, f\"test_{save_file_name}\")\n    print(\"finished test\")\n    \n    torch.save({\n        \"training_data\": train_ls, \n        \"validation_data\": valid_ls,\n        \"testing_data\": test_ls\n    }, save_file_name)\n    print(\"DONE\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"cache_dataset(dataset[\"train\"], {\n            \"train\": (0, 7000),\n            \"valid\": (7000, 10000),\n            \"test\": (10000, 11615)\n}, save_file_name=\"alldata.pth\")","metadata":{},"execution_count":null,"outputs":[]}]}